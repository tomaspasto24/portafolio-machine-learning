<!doctype html>
<html lang="zxx">

<head>
    <title>Portafolio Tomás Silva</title>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="description" content="" />
    <meta name="keywords" content=" " />
    <meta name="developer" content="Tomás Silva">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- FAV AND ICONS   -->
    <link rel="shortcut icon" href="../assets/images/favicon.ico">
    <link rel="shortcut icon" href="../assets/images/apple-icon.png">
    <link rel="shortcut icon" sizes="72x72" href="../assets/images/apple-icon-72x72.png">
    <link rel="shortcut icon" sizes="114x114" href="../assets/images/apple-icon-114x114.png">

    <!-- Font-->
    <link href="https://fonts.googleapis.com/css?family=Roboto&display=swap" rel="stylesheet">
    <!-- Font Awesome -->
    <link rel="stylesheet" href="../assets/icons/font-awesome-4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="../assets/icofont/icofont.min.css">
    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="../assets/plugins/css/bootstrap.min.css">
    <!-- Owl Carousel CSS-->
    <link rel="stylesheet" href="../assets/plugins/css/owl.css">
    <!-- Fancybox-->
    <link rel="stylesheet" href="../assets/plugins/css/jquery.fancybox.min.css">
    <!-- Anime CSS-->
    <link rel="stylesheet" href="../assets/plugins/css/revealer.css">
    <!-- Aos CSS-->
    <link rel="stylesheet" href="../assets/plugins/css/aos.css">
    <!-- Animate CSS-->
    <link rel="stylesheet" href="../assets/plugins/css/animate.css">
    <!-- Custom CSS-->
    <link rel="stylesheet" href="../assets/css/styles.css">
    <!-- Responsive -->
    <link rel="stylesheet" href="../assets/css/responsive.css">


</head>

<body class="black-bg body-2">
    <!--
    ===================
        NAVIGATION
    ===================
    -->
    <header class="black-bg mh-header nav-scroll fixed-top mh-xss-mobile-nav" id="zb-header">
        <div class="overlay"></div>
        <div class="container">
            <div class="row">
                <nav class="navbar navbar-expand-lg mh-nav nav-btn">
                    <a class="navbar-brand" href="index.html"><img src="../assets/images/logo.svg" alt=""
                            class="img-fluid"></a>
                    <button class="navbar-toggler" type="button" data-toggle="collapse"
                        data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                        aria-expanded="false" aria-label="Toggle navigation">
                        <span class="navbar-toggler-icon icon">
                            <i class="fa fa-bars"></i>
                        </span>
                    </button>
                    <div class="collapse navbar-collapse" id="navbarSupportedContent">
                        <ul class="navbar-nav mr-0 ml-auto">
                            <li class="nav-item">
                                <a class="nav-link" href="../index.html">Inicio</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link" href="./index.html">Marco teórico</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link" href="../ejercicios/index.html">Algoritmos</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link" href="../casos-estudio/index.html">Casos de estudio</a>
                            </li>
                        </ul>
                    </div>
                </nav>
            </div>
        </div>
    </header>
    <section class="section relative portfolio-home">
        <div class="container">
            <div class="row">
                <div class="col-md-7 mx-auto">
                    <div class="portfolio-title wow fadeInUp" data-wow-duration=".9s" data-wow-delay=".1s">
                        <h1>
                            Tratamiento previo de los datos y fundamentos para los algoritmos de ML
                        </h1>
                        <span></span>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="ag-blog-single-page">
        <div class="container">
            <div class="row section-separator vertical-middle-content ev-home-padding pb-0">
                <div class="col-sm-12 col-md-10">
                    <div class="ag-blog-post-content">
                        <div class="bl_img" id="rev-1">
                            <img src="../assets/images/data-prepare.jpg" alt="" class="img-fluid">
                        </div>
                        <h5>
                            Cómo funcionan los algoritmos machine learning
                        </h5>
                        <p>
                            Los algoritmos machine learning funcionan como una función de destino (f) que mapea una
                            variable X y produce una salida Y.

                            Y = f(X)

                            También existe un error (e), este puede ser como por ejemplo no tener suficientes atributos
                            para caracterizar de forma suficiente el mapeo de X a Y. Este error es llamado error
                            irreversible porque no importa lo bueno que seamos al estimando la función de destino (f) no
                            podremos reducir este error.

                            Y = f(X) + e
                        </p>
                        <p>
                            No estamos interesados en la forma de la función, solo que pueda hacer predicciones
                            precisas. Tenemos que aprender el mapeo de Y = f(X) para entender la relación con los datos,
                            esto es llamado Inferencia Estadística .

                            Cuando aprendemos una función (f) estamos estimando su forma a partir de los datos que
                            tenemos disponibles. Como tal, esta estimación tendrá un error. No será una estimación
                            perfecta para el mapeo hipotético subyacente de Y dado X. Se dedica mucho tiempo al
                            aprendizaje automático aplicado a intentar mejorar la estimación de la función subyacente y,
                            a largo plazo, mejorar el rendimiento de las predicciones hechas por el modelo.

                            Los algoritmos de aprendizaje automático son técnicas para estimar la función de destino (f)
                            para predecir la variable de salida (Y) dada las variables de entrada (X). Diferentes
                            representaciones hacen diferentes suposiciones sobre la forma de la función que se está
                            aprendiendo, como si es lineal o no lineal. Diferentes algoritmos de aprendizaje automático
                            hacen diferentes suposiciones sobre la forma y la estructura de la función y la mejor manera
                            de optimizar una representación para aproximarla. Esta es la razón por la que es tan
                            importante probar un conjunto de diferentes algoritmos en un problema de aprendizaje
                            automático, porque no podemos saber de antemano qué enfoque será el mejor para estimar la
                            estructura de la función subyacente que estamos tratando de aproximar.
                        </p>

                        <h5>
                            Algoritmos Paramétricos de ML
                        </h5>
                        <p>
                            Algoritmos que simplifican la función de forma conocida. Un modelo de aprendizaje que resume
                            los datos con un conjunto de parámetros de largo fijo.

                            Estos algoritmos llevan dos pasos:

                            - Seleccionar la forma de la función.
                            - Aprender los coeficientes de la función para el entrenamiento de datos.

                            Una forma fácil de entender es teniendo como mapping function una linea, como se utiliza en
                            la regresión lineal:

                            B0 + B1 × X1 + B2 × X2 = 0

                            B0, B1 y B2 son coeficientes de la linea que controlan la intersección y la pendiente,
                            mientras que X0 y X2 son variables de entrada. Asumiendo que la forma funcional de la linea
                            simplifica el proceso de aprendizaje. Ahora todo lo que tenemos que hacer es estimar el
                            coeficiente para la ecuación de la linea y tenemos un modelo predictivo para el problema.
                        </p>
                        <p>
                            A menudo, la forma funcional asumida es una combinación lineal de las variables de entrada
                            y, como tales, los algoritmos paramétricos de aprendizaje automático a menudo también se
                            llaman algoritmos lineales de aprendizaje automático. El problema es que la función
                            subyacente desconocida real puede no ser una función lineal como una línea. Podría ser casi
                            una línea y requerir una pequeña transformación de los datos de entrada para funcionar
                            correctamente. O podría no ser nada como una línea en la que la suposición es incorrecta y
                            el enfoque producirá malos resultados.
                        </p>
                        <p>
                            Algunos ejemplos de algoritmos parametricos incluyen:

                            - Regresión lógica
                            - Análisis discriminante linear
                            - Perceptron

                            Beneficios:

                            - Simple: metodos faciles de entender.
                            - Rápido : los modelos parametricos son muy rápidos para aprender acerca de los datos.
                            - Menos data: No requieren tantos datos de entrenamiento y pueden funcionar bien incluso si
                            el ajuste a los datos no es perfecto.

                            Limitaciones:

                            - Restringido: los métodos están restringidos a una forma especifica.
                            - Complejidad limitada: los métodos son adecuados para problemas fáciles .
                            - Mal ajuste: en la practica es poco probable que los métodos coincidan con la mapping
                            function.
                        </p>

                        <h5>
                            Algoritmos No Paramétricos de ML
                        </h5>
                        <p>
                            Son algoritmos que no asumen de forma fuerte la forma de la mapping function. Estos
                            algoritmos son libres de aprender cualquier forma funcional de los datos de entrenamiento.

                            Son buenos cuando tienes muchos datos y no tienes conocimientos previos. Y cuando no quieres
                            preocuparte demasiado por elegir buenas características.

                            Estos métodos buscan encajar mejor los datos de entrenamiento en la construcción de la
                            función de mapeo, al tiempo que mantiene cierta capacidad de generalizar datos no vistos.
                            Son capaces de adaptarse a un gran numero de formas funcionales.

                            Un modelo no paramétrico fácil de entender es el algoritmo de los vecinos más cercanos que
                            hace predicciones basadas en los patrones de entrenamiento más similares para una nueva
                            instancia de datos. El método no asume nada sobre la forma de la función de mapeo, aparte de
                            que es probable que los patrones que están cerca tengan una variable de salida similar.
                            Algunos ejemplos más de algoritmos populares de aprendizaje automático no paramétrico son:

                            - Decision Trees like CART and C4.5
                            - Naive Baye
                            - Support Vector Machines
                            - Neural Networks

                            Beneficios:

                            - Flexibilidad: capaz de adaptar un rango de formas funcionales.
                            - Poder: no asume (o asume de forma pobre) la función subyacente.
                            - Performance: puede resultar en una performance grande, rápida .

                            Limitaciones:

                            - Mas data: requiere mucha mas data para estimar la mapping function.
                            - Lento: mucho mas lento de entrenar ya que a menudo tiene muchos mas parámetros para
                            entrenar.
                            - Overfitting: Es más peligroso sobrepasar los datos de entrenamiento y es más difícil
                            explicar por qué se hacen predicciones específicas.
                        </p>

                        <h5>
                            Supervised, unsupervised and semi-supervised learning
                        </h5>

                        <h5>
                            Supervised Machine Learning
                        </h5>
                        <p>
                            La mayoría del machine learning es supervised learning. Supervised learning es cuando tenes
                            variables de entrada (X) y variable de salida (Y) y usas el algoritmo para mapear la funcion
                            desde las variables de entrada hasta las de salida.

                            Y =f(X)

                            El objetivo es aproximar el mapeo de la funcion tan bien que cuando tengas unas nuevas
                            variables de entrada (X), el algoritmo pueda predecir la variable de salida (Y).

                            El aprendizaje termina cuando el algoritmo logra un nivel aceptable de performance.

                            Supervised learning se puede agrupar en problemas de Clasificacion y Regresion.

                            Clasificacion → es cuando la variable de salida es una categoria (rojo, azul, amarillo).

                            Regresion → es cuando la variable de salida es un valor real (dolares, peso, centigrados).
                        </p>

                        <h5>
                            Unsupervised Machine Learning
                        </h5>

                        <p>
                            Es cuando tienes las variables de entrada pero no las de salida. El objetivo del aprendizaje
                            no supervisado es modelar la estructura o distribución subyacente en los datos para obtener
                            más información sobre los datos.

                            Estos se llaman aprendizaje sin supervisión porque, a diferencia del aprendizaje supervisado
                            anterior, no hay respuestas correctas y no hay profesor. Los algoritmos se dejan a sus
                            propios dispositivos para descubrir y presentar la estructura interesante en los datos. Los
                            problemas de aprendizaje no supervisado se pueden agrupar aún más en problemas de Clustering
                            y Association.

                            Clustering → es cuando quieres descubrir las agrupaciones inherentes a los datos. Por
                            ejemplo agrupar a los clientes por comportamientos de compra.

                            Association → es cuando quieres descubrir reglas en tus datos. Por ejemplo el cliente que
                            compra A también compra B.

                            Algoritmos:

                            - k-means for clustering problems.
                            - Apriori algorithm for association rule learning problems.
                        </p>

                        <h5>
                            Semi-supervised Machine Learning
                        </h5>

                        <p>
                            Cuando tienes muchas variables de entrada (X) pero pocas de salida son etiquetadas (Y). Un
                            ejemplo es un archivo de fotos en la que cada foto esta, o no, etiquetada.

                            Algunos datos están etiquetados, pero la mayoría de los datos no están etiquetados y se
                            puede utilizar una mezcla de técnicas supervisadas y no supervisadas.
                        </p>

                        <h5>
                            Bias-Variance Trade-Off
                        </h5>

                        <p>
                            El error de predicción para cualquier algoritmo de Machine Learning se puede dividir en las
                            siguientes partes:

                            - Bias Error
                            - Variance Error
                            - Irreducible Error: es el error introducido a partir del encuadre del problema, puede
                            deberse a variables de entrada mal reconocidas.
                        </p>

                        <h5>
                            Bias Error (sesgo error)
                        </h5>
                        <p>
                            Bias son las suposiciones hechas por el modelo para hacer que la target function sea facil
                            de aprender.

                            - Low bias: supone más suposiciones sobre la forma de target function.
                            - High bias: supone menos suposiciones sobre la forma de target function.

                            Examples of low-bias machine learning algorithms include: Decision Trees, k-Nearest Neigh-
                            bors and Support Vector Machines. Examples of high-bias machine learning algorithms include:
                            Linear Regression, Linear Discriminant Analysis and Logistic Regression.
                        </p>

                        <h5>
                            Variance Error
                        </h5>

                        <p>
                            La variación es la cantidad que cambiará la estimación de la función objetivo si se utilizan
                            diferentes datos de entrenamiento.

                            - Low variance: Sugiere pequeños cambios en la estimación de la función objetivo con cambios
                            en el conjunto de datos de entrenamiento.
                            - High variance: Sugiere grandes cambios en la estimación de la función de destino con
                            cambios en el conjunto de datos de entrenamiento.

                            Algoritmos no parametrizados tienen mucha flexibilidad y alto sesgo (bias).

                            Ejemplos de baja varianza: Linear Regression, Linear Discriminant Analysis and Logistic
                            Regression. Examples of high-variance machine learning algorithms include: Decision Trees,
                            k-Nearest Neighbors and Support Vector Machines.
                        </p>

                        <h5>
                            Bias-Variance Trade-Off
                        </h5>

                        <p>
                            El objetivo de los algoritmos supervisados es lograr tener poca varianza y poco sesgo
                            (bias). A su vez el algoritmo debe lograr un buen rendimiento de prediccion.

                            En lineas generales:

                            - Algoritmos parametrizados o linales, suelen tener alto sesgo (high bias) pero baja
                            varianza (low variance).
                            - Algoritmos no parametrizados o no lineales, suelen tener bajo sesgo (low bias) pero alta
                            varianza (high variance).
                            - El algoritmo de los vecinos más cercanos tiene un bajo sesgo y una alta varianza, pero la
                            compensación se puede cambiar aumentando el valor de k, lo que aumenta el número de vecinos
                            que contribuyen a la predicción y, a su vez, aumenta el sesgo del modelo.
                            - El algoritmo de la máquina de vector de soporte tiene un sesgo bajo y una alta varianza,
                            pero la compensación se puede cambiar aumentando el parámetro C que influye en el número de
                            violaciones del margen permitido en los datos de entrenamiento, lo que aumenta el sesgo pero
                            disminuye la varianza.

                            Tanto sesgo como varianza son inversamente proporcionales, al aumentar uno baja el otro y
                            asi.
                        </p>


                        <h5>
                            Overfitting y Underfitting
                        </h5>

                        <h5> Generalization </h5>
                        <p>
                            La generalización se refiere a lo bien que los conceptos aprendidos por un modelo de
                            aprendizaje automático se aplican a ejemplos específicos que no vieron el modelo cuando
                            estaba aprendiendo. El objetivo de un buen modelo de aprendizaje automático es generalizar
                            bien de los datos de entrenamiento a cualquier dato del dominio problemático. Esto nos
                            permite hacer predicciones en el futuro sobre datos que el modelo nunca ha visto. Hay una
                            terminología utilizada en el aprendizaje automático cuando hablamos de lo bien que un modelo
                            de aprendizaje automático aprende y se generaliza a nuevos datos, a saber, overfitting y
                            underfitting. El sobreajuste y el infraajuste son las dos principales causas del bajo
                            rendimiento de los algoritmos de aprendizaje automático.
                        </p>


                        <h5> Statistical Fit </h5>
                        <p>
                            Se refiere a lo bien que se aproxima a la target function. Las estadísticas a menudo
                            describen la bondad del ajuste, que se refiere a las medidas utilizadas para estimar qué tan
                            bien la aproximación de la función coincide con la función de destino. Algunos de estos
                            métodos son útiles en el aprendizaje automático (por ejemplo, calcular los errores
                            residuales), pero algunas de estas técnicas asumen que conocemos la forma de la función de
                            destino a la que nos estamos acercando, lo que no es el caso en el aprendizaje automático.
                            Si conocimos la forma de la función objetivo, la usaríamos directamente para hacer
                            predicciones, en lugar de tratar de aprender una aproximación a partir de muestras de datos
                            de entrenamiento ruidosos.
                        </p>


                        <h5> Overfitting </h5>
                        <p>
                            El Overfitting se refiere a un modelo que modela demasiado bien los datos de entrenamiento.
                            El Overfitting ocurre cuando un modelo aprende el detalle y el ruido en los datos de
                            entrenamiento en la medida en que afecta negativamente al rendimiento del modelo en los
                            nuevos datos. Esto significa que el ruido o las fluctuaciones aleatorias en los datos de
                            entrenamiento son recogidos y aprendidos como conceptos por el modelo. El problema es que
                            estos conceptos no se aplican a los nuevos datos y tienen un impacto negativo en la
                            capacidad de los modelos para generalizar. El Overfitting es más probable con modelos no
                            paramétricos y no lineales que tienen más flexibilidad al aprender una función objetivo.
                            Como tal, muchos algoritmos de aprendizaje automático no paramétricos también incluyen
                            parámetros o técnicas para limitar y restringir la cantidad de detalles que aprende el
                            modelo. Por ejemplo, los árboles de decisión son un algoritmo de aprendizaje automático no
                            paramétrico que es muy flexible y está sujeto a datos de entrenamiento de Overfitting. Este
                            problema se puede abordar podando un árbol después de que haya aprendido con el fin de
                            eliminar algunos de los detalles que ha recogido.
                        </p>


                        <h5> Underfitting </h5>
                        <p>
                            El Underfitting se refiere a un modelo que no puede modelar los datos de entrenamiento ni
                            generalizar a nuevos datos. Un modelo de aprendizaje automático insuficiente no es un modelo
                            adecuado y será obvio, ya que tendrá un bajo rendimiento en los datos de entrenamiento. El
                            ajuste inferior a menudo no se discute, ya que es fácil de detectar dada una buena métrica
                            de rendimiento. El remedio es seguir adelante y probar algoritmos alternativos de
                            aprendizaje automático. Sin embargo, proporciona un buen contraste con el problema del
                            concepto de Overfitting.
                        </p>


                        <h5> Un buen ajuste en el aprendizaje automático </h5>
                        <p>
                            Idealmente el objetivo es buscar un modelo optimo entre overfitting y underfitting, pero
                            esto es my dificil en la practica.
                        </p>

                        <p>
                            Para entender este objetivo, podemos ver el rendimiento de un algoritmo de aprendizaje
                            automático a lo largo del tiempo, ya que está aprendiendo datos de entrenamiento. Podemos
                            trazar
                            tanto la habilidad en los datos de entrenamiento como la habilidad en un conjunto de datos
                            de
                            prueba que hemos retenido del proceso de entrenamiento. Con el tiempo, a medida que el
                            algoritmo
                            aprende, el error del modelo en los datos de entrenamiento disminuye, al igual que el error
                            en
                            el conjunto de datos de prueba. Si entrenamos durante demasiado tiempo, el rendimiento en el
                            conjunto de datos de entrenamiento puede seguir disminuyendo porque el modelo se está
                            sobreadaptando y aprendiendo el detalle y el ruido irrelevantes en el conjunto de datos de
                            entrenamiento. Al mismo tiempo, el error del conjunto de pruebas comienza a aumentar de
                            nuevo a
                            medida que disminuye la capacidad del modelo para generalizar. El punto óptimo es el punto
                            justo
                            antes de que el error en el conjunto de datos de prueba comience a aumentar, donde el modelo
                            tiene una buena habilidad tanto en el conjunto de datos de entrenamiento como en el conjunto
                            de
                            datos de prueba no visto. Puedes realizar este experimento con tus algoritmos de aprendizaje
                            automático favoritos. Esto a menudo no es técnica útil en la práctica, porque al elegir el
                            punto
                            de parada para el entrenamiento utilizando la habilidad en el conjunto de datos de la
                            prueba,
                            significa que el conjunto de pruebas ya no es inveo o una medida objetiva independiente.
                            Algunos
                            conocimientos (un montón de conocimientos útiles) sobre esos datos se han filtrado en el
                            procedimiento de entrenamiento. Hay dos técnicas adicionales que puede utilizar para ayudar
                            a
                            encontrar el punto óptimo en la práctica: métodos de remuestreo y un conjunto de datos de
                            validación.
                        </p>
                        <h5> Como eliminar el Overfitting </h5>
                        <p>
                            Tanto el overfitting como el underfitting pueden conducir a un mal rendimiento del modelo.
                            Pero, con mucho, el problema más común en el aprendizaje automático aplicado es el
                            sobreajuste. El sobreajuste es un problema porque la evaluación de los algoritmos de
                            aprendizaje automático en los datos de entrenamiento es diferente de la evaluación que más
                            nos importa, es decir, qué tan bien se desempeña el algoritmo en datos no se ven. Hay dos
                            técnicas importantes que puedes usar al evaluar los algoritmos de aprendizaje automático
                            para limitar el sobreajuste: 1. Utilice una técnica de remuestreo para estimar la precisión
                            del modelo. 2. Retener un conjunto de datos de validación. La técnica de remuestreo más
                            popular es la validación cruzada de k-fold. Le permite entrenar y probar su modelo k-times
                            en diferentes subconjuntos de datos de entrenamiento y construir una estimación del
                            rendimiento de un modelo de aprendizaje automático sobre datos no se ven. Un conjunto de
                            datos de validación es simplemente un subconjunto de sus datos de entrenamiento que retiene
                            de sus algoritmos de aprendizaje automático hasta el final de su proyecto. Después de haber
                            seleccionado y ajustado sus algoritmos de aprendizaje automático en su conjunto de datos de
                            entrenamiento, puede evaluar los modelos aprendidos en el conjunto de datos de validación
                            para obtener una idea objetiva final de cómo podrían funcionar los modelos en datos no
                            vistos. El uso de la validación cruzada es un estándar de oro en el aprendizaje automático
                            aplicado para estimar la precisión del modelo en datos no vistos. Si tienes los datos, usar
                            un conjunto de datos de validación también es una excelente práctica.
                        </p>


                        <h5>Preprocessing Data</h5>
                        <p>
                            Es el proceso que mas lleva en el análisis de datos. Existen dos tipos:

                            - *Blending* (mezcla) se trata de transformar un conjunto de datos de un estado a otro o
                            combinar múltiples conjuntos de datos.
                            - Filter
                            - Select
                            - *Cleansing (limpieza)* se trata de mejorar los datos para que el modelado entregue mejores
                            resultados.
                            - Replace
                            - Normalize
                            - Eliminar valores atípicos que se generan por mediciones incorrectas. En algunos análisis
                            pueden ser importantes (por ejemplo identificar fraude). Pero en la mayoría de los casos en
                            los
                            que se realiza un análisis, no.
                            - Detect Outliners → busca 10 ejemplos mas alejados y los marca como valores atipicos.
                        </p>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <script src="../assets/plugins/js/jquery.min.js"></script>
    <!-- popper -->
    <script src="../assets/plugins/js/popper.min.js"></script>
    <!-- bootstrap -->
    <script src="../assets/plugins/js/bootstrap.min.js"></script>
    <!-- waypoint  -->
    <script src="../assets/plugins/js/waypoints.min.js"></script>
    <!-- owl carousel -->
    <script src="../assets/plugins/js/owl.carousel.js"></script>
    <!-- validator -->
    <script src="../assets/plugins/js/validator.min.js"></script>
    <!-- wow -->
    <script src="../assets/plugins/js/wow.min.js"></script>
    <!-- jquery nav -->
    <script src="../assets/plugins/js/jquery.nav.js"></script>
    <!-- Isotop -->
    <script src="../assets/plugins/js/isotope.pkgd.js"></script>
    <script src="../assets/plugins/js/packery-mode.pkgd.js"></script>
    <!-- Fancybox js-->
    <script src="../assets/plugins/js/jquery.fancybox.min.js"></script>
    <!-- AOS  -->
    <script src="../assets/plugins/js/aos.js"></script>
    <script src="../assets/plugins/js/wow.min.js"></script>
    <script src="../assets/plugins/js/TweenMax.min.js"></script>
    <!-- Parallax -->
    <script src="../assets/plugins/js/simpleParallax.min.js"></script>
    <!-- <script src="../assets/plugins/js/typed.js"></script> -->
    <!-- Scroll Effect -->
    <script src="../assets/plugins/js/anime.min.js"></script>
    <script src="../assets/plugins/js/scrollMonitor.js"></script>
    <script src="../assets/plugins/js/scroll-effect.js"></script>
    <!-- Custom Scripts-->
    <script src="../assets/js/custom-scripts.js"></script>
    <script>
        new WOW().init();
    </script>
</body>

</html>